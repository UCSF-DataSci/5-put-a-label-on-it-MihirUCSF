# Assignment 5: Health Data Classification Results

This file contains your manual interpretations and analysis of the model results from the different parts of the assignment.

## Part 1: Logistic Regression on Imbalanced Data

### Interpretation of Results

Based off of the Logistic Regression Model, the AUC metric perfored the best with a value of 0.8853. The AUC was able to measure the model's ability to rank positive instances higher than negative ones, regardless of classification threshold, and is robust to class imbalance. A value of 0.8853 indicates strong discriminative power; this model is good at distinguishing between disease and non-disease cases based on predicted probabilities.

The precision metric performed the worst (0.3142). This indicates that only 31.4% of the cases predicted as positive were actually positive, the matrix shows 251 false positives and 115 true positives. This is also due to fact that our data is imbalanced, the model is biased towards predicting the minority class to accomodate for the imbalance which results in more false positives.

The class imbalanced impact score was 0.4413 which is considered moderate impact. This is also shown by the difference between overall accuracy (81.0%) and precision (31.4%). The confusion matrix also shows a skewed prediction pattern of 251 false positives versus 27 false negatives. The model tried to compensate for the imbalance by improving recall at the cost of precision. 

The confusion matrix (TN: 1073, FP: 251, FN: 27, TP: 115) shows several key points. The model has strong recall (sensitivity) which means it correctly identified 115 out of 142 positive cases (81.0%). It has poor specificity, the model incorrectly flagged 251 of 1324 negative cases as positive (1073/1324 -> 81.0%). This creates a classification bias wherein the model is using the 'balanced' parameter classifying more cases than it should be as positive. This indicates that the classification threshold should be adjusted to improve the balance between precision and recall.  

## Part 2: Tree-Based Models with Time Series Features

### Comparison of Random Forest and XGBoost

XGBoost outperformed Random Forest according to the AUC score by 0.53% improvement. The XGBoost model was able to better separate the positive and negative classes in the test set based on predicted probabilities. 

The XGBoost model was better for several reasons. This model builds trees sequentially in which each new tree corrects errors made by previous ones; this is effective for capturing the gradient structure in time-series data, where temporal patterns evolve progressively. XGBoost gave highest importance to hr_rolling_mean (22.29%), while Random Forest emphasized diastolic_bp (20.88%). XGBoost's emphasis on the time-series feature suggests it better leveraged the temporal patterns. XGBoost has built-in L1 and L2 regularization that helps prevent overfitting, especially important when dealing with complex time-series features that might introduce noise. XGBoost's scale_pos_weight parameter was specifically tuned to address the class imbalance (90.3% negative, 9.7% positive), helping it better identify the minority class patterns. Lastly, XGBoost's sequential learning was able to capture subtle interactions between time-varying features and static features. 

The addition of time-series features had a positive impact on model performance. Both models ranked time-series features among their top 5 most important features, XGBoost, hr_rolling_mean was the first most important feature (22.29%), and for Random Forest, hr_rolling_std was third (15.28%) and hr_rolling_mean was fifth (10.49%). The time-series features allowed the models to detect patterns in heart rate variability over time, which point-in-time measurements alone couldn't reveal. This in turn adds context about how a patient's condition was evolving. The high AUC scores suggest that these temporal features significantly improved the models' ability to distinguish between positive and negative cases. The rolling mean captured the central tendency of heart rate over time, while the standard deviation captured variability/instability - both providing information that likely correlates with disease outcomes. Lastly, by processing each patient separately when creating the rolling features, the models could detect individual deviations from personal baselines rather than population averages, increasing sensitivity to patient-specific changes that might indicate disease.

## Part 3: Logistic Regression with Balanced Data

### Improvement Analysis

Precision showed the most dramatic improvement, increasing from 31.42% to 38.39%, representing a +22.18% relative improvement. This means the SMOTE-balanced model generates significantly fewer false positives - an improvement for clinical applications where false alarms can lead to unnecessary treatments or anxiety.
F1 Score showed the second largest improvement, from 45.28% to 52.65%, a +16.28% relative improvement. This balanced measure indicates better overall performance across both precision and recall.

Recall showed the smallest improvement, from 80.99% to 83.80%, a +3.47% relative improvement. The imbalanced model already had fairly good recall due to its class_weight='balanced' parameter, which prioritized finding minority class cases. AUC showed the second smallest improvement, from 88.53% to 91.49%, a +3.34% relative improvement. While this is a smaller percentage improvement, achieving an AUC over 0.91 represents good discriminative ability.

Different metrics improved for various reasons. Metrics that were already strong in the imbalanced model (like recall and AUC) had less room for improvement compared to precision, which started at a low value. SMOTE creates synthetic minority class examples that help the model better understand the decision boundary between classes. This particularly helps precision by reducing false positives. In imbalanced classification, there is a trade-off between precision and recall. By providing balanced training data, SMOTE helps the model find a better operating point that improves both, but precision had more to gain. In part 1, class_weight='balanced' was used which artificially weights minority class errors during training. SMOTE creates new synthetic examples, giving the model more information about minority class patterns. The improvement in the confusion matrix (reducing false positives from 251 to 191) shows that SMOTE helped correct the model's bias toward over-predicting the positive class.

Class imbalanced should be addressed because it improves all perfrmance metrics simutaneously resulting in improved model quality, the accuracy metric in imbalanced data is misleading, it reduces false alarms for clinical practices, SMOTE also provided a balance between identifying disease cases and prediction accuracy, and the coefficients in the SMOTE model are trained on balanced data not artificially weighted data. 
## Overall Conclusions

The severe class imbalance (90.3% negative, 9.7% positive) was the most significant challenge, causing poor precision (31.4% in Part 1) and biasing models toward over-predicting the positive class. Time-series features extracted from heart rate data provided crucial predictive power. XGBoost ranked hr_rolling_mean as its most important feature (22.3%), showing that patient trends over time contained valuable information beyond static measurements. Diastolic blood pressure emerged as consistently important across models, ranking highest in Random Forest (20.9%) and showing a strong coefficient (0.1444) in the SMOTE-balanced logistic regression. Smoking status provided significant predictive value when properly encoded. In the SMOTE model, being a smoker increased disease risk substantially (coefficient: +0.3608) while non-smokers had reduced risk (-0.2996).

Applying SMOTE improved all metrics, with precision showing the most dramatic improvement (+22.2%). By generating synthetic minority samples rather than just weighting, SMOTE provided the model with better understanding of decision boundaries. Adding rolling statistics (mean and standard deviation) of heart rate created powerful predictive features that captured temporal patterns invisible in static measurements, leading to exceptionally high AUC scores. Properly encoding categorical variables like smoking status preserved valuable information that contributed significantly to model performance, as these variables were the top features. Calculating rolling features separately for each patient allowed the models to detect deviations from personal baselines rather than population averages, increasing sensitivity to patient-specific changes.

To improve future modeling: perform feature expansion (extract additional time-series features from other physiological measurements, create interaction terms between time-varying and static features), consider hierarchical models that account for patient-specific baselines, implement precision-recall curve analysis to find optimal classification thresholds, test on completely separate patient populations to ensure generalizability, implement SHAP (SHapley Additive exPlanations) values to better understand feature contributions, and include additional biomarkers that might correlate with disease progression. 