{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67dec2bf",
   "metadata": {},
   "source": [
    "# Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16534925",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc398636",
   "metadata": {},
   "source": [
    "# Part 1: Introduction to Classification & Evaluation\n",
    "\n",
    "**Objective:** Load the synthetic health data, train a Logistic Regression model, and evaluate its performance.\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f695309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17c655f",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Implement the `load_data` function to read the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad93e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    # check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist. Please run generate_data.py first.\")\n",
    "    \n",
    "    # load the CSV file using pandas\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # display basic information about the loaded data\n",
    "    print(f\"Loaded {df.shape[0]} records with {df.shape[1]} features.\")\n",
    "    print(f\"Features: {', '.join(df.columns)}\")\n",
    "    print(f\"Number of unique patients: {df['patient_id'].nunique()}\")\n",
    "    print(f\"Target distribution:\\n{df['disease_outcome'].value_counts(normalize=True).rename('proportion')}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319a68a8",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "Implement `prepare_data_part1` to select features, split data, and handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc9161ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_part1(df, test_size=0.2, random_state=42):\n",
    "    # select relevant features\n",
    "    selected_features = ['age', 'systolic_bp', 'diastolic_bp', 'glucose_level', 'bmi']\n",
    "    X = df[selected_features]\n",
    "    \n",
    "    # select target variable\n",
    "    y = df['disease_outcome']\n",
    "    \n",
    "    # split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # handle missing values using SimpleImputer\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train = pd.DataFrame(\n",
    "        imputer.fit_transform(X_train),\n",
    "        columns=X_train.columns,\n",
    "        index=X_train.index\n",
    "    )\n",
    "    \n",
    "    X_test = pd.DataFrame(\n",
    "        imputer.transform(X_test),\n",
    "        columns=X_test.columns,\n",
    "        index=X_test.index\n",
    "    )\n",
    "    \n",
    "    print(\"Missing values before imputation:\")\n",
    "    print(f\"  Training set: {df[selected_features].iloc[X_train.index].isna().sum().sum()}\")\n",
    "    print(f\"  Testing set: {df[selected_features].iloc[X_test.index].isna().sum().sum()}\")\n",
    "    print(\"Missing values after imputation:\")\n",
    "    print(f\"  Training set: {X_train.isna().sum().sum()}\")\n",
    "    print(f\"  Testing set: {X_test.isna().sum().sum()}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fb970d",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "Implement `train_logistic_regression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "554ce3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X_train, y_train):\n",
    "    # initialize the logistic regression model\n",
    "    model = LogisticRegression(\n",
    "        solver='liblinear',  \n",
    "        class_weight='balanced',  \n",
    "        random_state=42,  \n",
    "        max_iter=1000  \n",
    "    )\n",
    "    \n",
    "    # train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # model coefficients to understand feature importance\n",
    "    print(\"Logistic Regression Model Trained Successfully\")\n",
    "    print(\"\\nModel Coefficients:\")\n",
    "    for feature, coef in zip(X_train.columns, model.coef_[0]):\n",
    "        print(f\"{feature}: {coef:.4f}\")\n",
    "    \n",
    "    print(f\"\\nIntercept: {model.intercept_[0]:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca130e8a",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "Implement `calculate_evaluation_metrics` to assess the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "920f8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_evaluation_metrics(model, X_test, y_test):\n",
    "    # generate predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]  # probability of positive class\n",
    "    \n",
    "    # calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # print confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"TN: {cm[0][0]}, FP: {cm[0][1]}\")\n",
    "    print(f\"FN: {cm[1][0]}, TP: {cm[1][1]}\")\n",
    "    \n",
    "    # return metrics in a dictionary\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "    \n",
    "    print(\"\\nModel Evaluation Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        if metric != 'confusion_matrix':\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1a9ace",
   "metadata": {},
   "source": [
    "## 6. Save Results\n",
    "\n",
    "Save the calculated metrics to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f314aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory and save metrics\n",
    "def save_results(metrics, file_path='results/results_part1.txt'):\n",
    "    # create 'results' directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # format metrics as strings\n",
    "    lines = [\n",
    "        \"LOGISTIC REGRESSION MODEL EVALUATION\",\n",
    "        \"=\" * 40,\n",
    "        f\"Accuracy:  {metrics['accuracy']:.4f}\",\n",
    "        f\"Precision: {metrics['precision']:.4f}\",\n",
    "        f\"Recall:    {metrics['recall']:.4f}\",\n",
    "        f\"F1 Score:  {metrics['f1_score']:.4f}\",\n",
    "        f\"AUC:       {metrics['auc']:.4f}\",\n",
    "        \"\\nCONFUSION MATRIX\",\n",
    "        \"=\" * 40,\n",
    "        f\"TN: {metrics['confusion_matrix'][0][0]}, FP: {metrics['confusion_matrix'][0][1]}\",\n",
    "        f\"FN: {metrics['confusion_matrix'][1][0]}, TP: {metrics['confusion_matrix'][1][1]}\",\n",
    "        \"\\nEvaluation completed on: \" + pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    ]\n",
    "    \n",
    "    # write metrics to file\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write('\\n'.join(lines))\n",
    "    \n",
    "    print(f\"\\nResults saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71467f0",
   "metadata": {},
   "source": [
    "## 7. Main Execution\n",
    "\n",
    "Run the complete workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bc3d88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7326 records with 10 features.\n",
      "Features: patient_id, timestamp, age, systolic_bp, diastolic_bp, glucose_level, bmi, smoker_status, heart_rate, disease_outcome\n",
      "Number of unique patients: 150\n",
      "Target distribution:\n",
      "disease_outcome\n",
      "0    0.902812\n",
      "1    0.097188\n",
      "Name: proportion, dtype: float64\n",
      "Training set: 5860 samples\n",
      "Testing set: 1466 samples\n",
      "Missing values before imputation:\n",
      "  Training set: 600\n",
      "  Testing set: 135\n",
      "Missing values after imputation:\n",
      "  Training set: 0\n",
      "  Testing set: 0\n",
      "Logistic Regression Model Trained Successfully\n",
      "\n",
      "Model Coefficients:\n",
      "age: 0.0105\n",
      "systolic_bp: 0.0194\n",
      "diastolic_bp: 0.1538\n",
      "glucose_level: 0.0525\n",
      "bmi: -0.1352\n",
      "\n",
      "Intercept: -18.1845\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 1073, FP: 251\n",
      "FN: 27, TP: 115\n",
      "\n",
      "Model Evaluation Metrics:\n",
      "accuracy: 0.8104\n",
      "precision: 0.3142\n",
      "recall: 0.8099\n",
      "f1_score: 0.4528\n",
      "auc: 0.8853\n",
      "\n",
      "Results saved to results/results_part1.txt\n",
      "\n",
      "Results Interpretation:\n",
      "best_metric: auc\n",
      "worst_metric: precision\n",
      "imbalance_impact_score: 0.4413\n",
      "imbalance_assessment: Moderate impact from class imbalance\n",
      "suggestion: Consider techniques to improve precision, such as feature engineering or ensemble methods\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1. Load data\n",
    "    data_file = 'data/synthetic_health_data.csv'\n",
    "    df = load_data(data_file)\n",
    "    \n",
    "    # 2. Prepare data\n",
    "    X_train, X_test, y_train, y_test = prepare_data_part1(df)\n",
    "    \n",
    "    # 3. Train model\n",
    "    model = train_logistic_regression(X_train, y_train)\n",
    "    \n",
    "    # 4. Evaluate model\n",
    "    metrics = calculate_evaluation_metrics(model, X_test, y_test)\n",
    "    \n",
    "    # 5. Save results\n",
    "    save_results(metrics)\n",
    "    \n",
    "    # 6. Interpret results\n",
    "    interpretation = interpret_results(metrics)\n",
    "    print(\"\\nResults Interpretation:\")\n",
    "    for key, value in interpretation.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab577b7",
   "metadata": {},
   "source": [
    "## 8. Interpret Results\n",
    "\n",
    "Implement a function to analyze the model performance on imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24c95611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_results(metrics):\n",
    "    # consider these metrics (exclude confusion_matrix)\n",
    "    metric_keys = ['accuracy', 'precision', 'recall', 'f1_score', 'auc']\n",
    "    metric_values = {k: metrics[k] for k in metric_keys}\n",
    "    \n",
    "    # determine which metric performed best and worst\n",
    "    best_metric = max(metric_values, key=metric_values.get)\n",
    "    worst_metric = min(metric_values, key=metric_values.get)\n",
    "    \n",
    "    # calculate an imbalance impact score\n",
    "    min_recall_f1 = min(metrics['recall'], metrics['f1_score'])\n",
    "    if metrics['accuracy'] > 0:  # Avoid division by zero\n",
    "        imbalance_impact = 1 - (min_recall_f1 / metrics['accuracy'])\n",
    "        imbalance_impact = min(1.0, max(0.0, imbalance_impact))  # Ensure between 0 and 1\n",
    "    else:\n",
    "        imbalance_impact = 1.0\n",
    "    \n",
    "    # return the results as a dictionary\n",
    "    interpretation = {\n",
    "        'best_metric': best_metric,\n",
    "        'worst_metric': worst_metric,\n",
    "        'imbalance_impact_score': round(imbalance_impact, 4)\n",
    "    }\n",
    "    \n",
    "    # add interpretation text based on the impact score\n",
    "    if imbalance_impact < 0.2:\n",
    "        interpretation['imbalance_assessment'] = \"Low impact from class imbalance\"\n",
    "    elif imbalance_impact < 0.5:\n",
    "        interpretation['imbalance_assessment'] = \"Moderate impact from class imbalance\"\n",
    "    else:\n",
    "        interpretation['imbalance_assessment'] = \"Severe impact from class imbalance\"\n",
    "    \n",
    "    # add suggestion based on metrics\n",
    "    if metrics['recall'] < 0.7:\n",
    "        interpretation['suggestion'] = \"Consider techniques to improve recall, such as adjusting the classification threshold or using more advanced resampling methods\"\n",
    "    elif metrics['precision'] < 0.7:\n",
    "        interpretation['suggestion'] = \"Consider techniques to improve precision, such as feature engineering or ensemble methods\"\n",
    "    else:\n",
    "        interpretation['suggestion'] = \"Model performance is good across metrics\"\n",
    "    \n",
    "    return interpretation"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
