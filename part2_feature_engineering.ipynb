{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12799627",
   "metadata": {},
   "source": [
    "# Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a6782f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a1bdd",
   "metadata": {},
   "source": [
    "# Part 2: Time Series Features & Tree-Based Models\n",
    "\n",
    "**Objective:** Extract basic time-series features from heart rate data, train Random Forest and XGBoost models, and compare their performance.\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25fb03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e026f710",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b378fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    # check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist. Please run generate_data.py first.\")\n",
    "    \n",
    "    # load the CSV file using pandas and parse timestamps\n",
    "    df = pd.read_csv(file_path, parse_dates=['timestamp'])\n",
    "    \n",
    "    # display basic information about the loaded data\n",
    "    print(f\"Loaded {df.shape[0]} records with {df.shape[1]} features.\")\n",
    "    print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "    print(f\"Number of unique patients: {df['patient_id'].nunique()}\")\n",
    "    print(f\"Heart rate range: {df['heart_rate'].min()} to {df['heart_rate'].max()} bpm\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0141c9bd",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Implement `extract_rolling_features` to calculate rolling mean and standard deviation for the `heart_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b3910fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rolling_features(df, window_size_seconds):\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # sort data by patient_id and timestamp\n",
    "    df_sorted = df_result.sort_values(['patient_id', 'timestamp'])\n",
    "    \n",
    "    # create empty columns for our rolling statistics\n",
    "    df_sorted['hr_rolling_mean'] = np.nan\n",
    "    df_sorted['hr_rolling_std'] = np.nan\n",
    "    \n",
    "    # process each patient separately to avoid mixing data between patients\n",
    "    for patient_id in df_sorted['patient_id'].unique():\n",
    "        patient_data = df_sorted[df_sorted['patient_id'] == patient_id].copy()\n",
    "        \n",
    "        # set timestamp as index\n",
    "        patient_data = patient_data.set_index('timestamp')\n",
    "        \n",
    "        # calculate rolling mean and standard deviation\n",
    "        rolling_window = patient_data['heart_rate'].rolling(window=f'{window_size_seconds}s')\n",
    "        \n",
    "        # calculate statistics on this window\n",
    "        patient_data['hr_rolling_mean'] = rolling_window.mean()\n",
    "        patient_data['hr_rolling_std'] = rolling_window.std()\n",
    "        \n",
    "        # reset index to bring timestamp back as a column\n",
    "        patient_data = patient_data.reset_index()\n",
    "        \n",
    "        # update the values in the original dataframe\n",
    "        idx = df_sorted['patient_id'] == patient_id\n",
    "        df_sorted.loc[idx, 'hr_rolling_mean'] = patient_data['hr_rolling_mean'].values\n",
    "        df_sorted.loc[idx, 'hr_rolling_std'] = patient_data['hr_rolling_std'].values\n",
    "    \n",
    "    # handle NaN values (from the start of each patient's time series)\n",
    "    df_sorted['hr_rolling_mean'] = df_sorted.groupby('patient_id')['hr_rolling_mean'].fillna(method='ffill')\n",
    "    df_sorted['hr_rolling_std'] = df_sorted.groupby('patient_id')['hr_rolling_std'].fillna(method='ffill')\n",
    "    \n",
    "    for patient_id in df_sorted['patient_id'].unique():\n",
    "        patient_mask = df_sorted['patient_id'] == patient_id\n",
    "        \n",
    "        # calculate patient's overall mean and std for heart rate\n",
    "        hr_mean = df_sorted.loc[patient_mask, 'heart_rate'].mean()\n",
    "        hr_std = df_sorted.loc[patient_mask, 'heart_rate'].std()\n",
    "        \n",
    "        mean_mask = patient_mask & df_sorted['hr_rolling_mean'].isna()\n",
    "        std_mask = patient_mask & df_sorted['hr_rolling_std'].isna()\n",
    "        \n",
    "        df_sorted.loc[mean_mask, 'hr_rolling_mean'] = hr_mean\n",
    "        df_sorted.loc[std_mask, 'hr_rolling_std'] = hr_std if not np.isnan(hr_std) else 0\n",
    "    \n",
    "    # feature summary\n",
    "    print(f\"Generated rolling features with {window_size_seconds} second window:\")\n",
    "    print(f\"  - hr_rolling_mean range: {df_sorted['hr_rolling_mean'].min():.1f} to {df_sorted['hr_rolling_mean'].max():.1f}\")\n",
    "    print(f\"  - hr_rolling_std range: {df_sorted['hr_rolling_std'].min():.1f} to {df_sorted['hr_rolling_std'].max():.1f}\")\n",
    "    print(f\"  Missing values after processing: {df_sorted[['hr_rolling_mean', 'hr_rolling_std']].isna().sum().sum()}\")\n",
    "    \n",
    "    return df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5531c3f",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "\n",
    "Implement `prepare_data_part2` using the newly engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90526d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_part2(df_with_features, test_size=0.2, random_state=42):\n",
    "    # select relevant features including the rolling features\n",
    "    selected_features = [\n",
    "        'age', 'systolic_bp', 'diastolic_bp', 'glucose_level', \n",
    "        'bmi', 'heart_rate', 'hr_rolling_mean', 'hr_rolling_std'\n",
    "    ]\n",
    "    \n",
    "    # add smoker_status as dummy variables if it exists in the dataframe\n",
    "    if 'smoker_status' in df_with_features.columns:\n",
    "        # convert smoker_status to dummy variables (one-hot encoding)\n",
    "        smoker_dummies = pd.get_dummies(df_with_features['smoker_status'], prefix='smoker')\n",
    "        \n",
    "        # add the dummy columns to the dataframe\n",
    "        df_with_features = pd.concat([df_with_features, smoker_dummies], axis=1)\n",
    "        \n",
    "        # add the dummy column names to the selected features\n",
    "        selected_features.extend(smoker_dummies.columns.tolist())\n",
    "    \n",
    "    X = df_with_features[selected_features]\n",
    "    \n",
    "    # select target variable\n",
    "    y = df_with_features['disease_outcome']\n",
    "    \n",
    "    # split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples with {X_train.shape[1]} features\")\n",
    "    print(f\"Testing set: {X_test.shape[0]} samples with {X_test.shape[1]} features\")\n",
    "    \n",
    "    # handle missing values using SimpleImputer\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train = pd.DataFrame(\n",
    "        imputer.fit_transform(X_train),\n",
    "        columns=X_train.columns,\n",
    "        index=X_train.index\n",
    "    )\n",
    "    \n",
    "    X_test = pd.DataFrame(\n",
    "        imputer.transform(X_test),\n",
    "        columns=X_test.columns,\n",
    "        index=X_test.index\n",
    "    )\n",
    "    \n",
    "    print(\"Missing values before imputation:\")\n",
    "    print(f\"  Training set: {df_with_features[selected_features].iloc[X_train.index].isna().sum().sum()}\")\n",
    "    print(f\"  Testing set: {df_with_features[selected_features].iloc[X_test.index].isna().sum().sum()}\")\n",
    "    print(\"Missing values after imputation:\")\n",
    "    print(f\"  Training set: {X_train.isna().sum().sum()}\")\n",
    "    print(f\"  Testing set: {X_test.isna().sum().sum()}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daff161",
   "metadata": {},
   "source": [
    "## 5. Random Forest Model\n",
    "\n",
    "Implement `train_random_forest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac5a73b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(X_train, y_train, n_estimators=100, max_depth=10, random_state=42):\n",
    "    # initialize the Random Forest classifier with specified parameters\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        class_weight='balanced',  # handle class imbalance\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1  # use all available cores for faster training\n",
    "    )\n",
    "    \n",
    "    # train the model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # print feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Random Forest Model Trained Successfully\")\n",
    "    print(f\"\\nTop 5 important features:\")\n",
    "    for i, (feature, importance) in enumerate(zip(feature_importance['feature'].values[:5], \n",
    "                                                 feature_importance['importance'].values[:5])):\n",
    "        print(f\"{i+1}. {feature}: {importance:.4f}\")\n",
    "    \n",
    "    return rf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb02f5f",
   "metadata": {},
   "source": [
    "## 6. XGBoost Model\n",
    "\n",
    "Implement `train_xgboost`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c65e3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(X_train, y_train, n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42):\n",
    "    # initialize the XGBoost classifier with specified parameters\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state,\n",
    "        use_label_encoder=False,  \n",
    "        eval_metric='logloss',    \n",
    "        scale_pos_weight=(len(y_train) - sum(y_train)) / sum(y_train) \n",
    "    )\n",
    "    \n",
    "    # train the model\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # get feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"XGBoost Model Trained Successfully\")\n",
    "    print(f\"\\nTop 5 important features:\")\n",
    "    for i, (feature, importance) in enumerate(zip(feature_importance['feature'].values[:5], \n",
    "                                               feature_importance['importance'].values[:5])):\n",
    "        print(f\"{i+1}. {feature}: {importance:.4f}\")\n",
    "    \n",
    "    return xgb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c565648",
   "metadata": {},
   "source": [
    "## 7. Model Comparison\n",
    "\n",
    "Calculate and compare AUC scores for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d20e72af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(models, X_test, y_test):\n",
    "    results = {}\n",
    "    \n",
    "    print(\"Model Comparison:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # calculate AUC for each model\n",
    "    for name, model in models.items():\n",
    "        # generate probability predictions\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # calculate AUC score\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "        results[name] = auc\n",
    "        \n",
    "        print(f\"{name} AUC: {auc:.4f}\")\n",
    "    \n",
    "    best_model = max(results, key=results.get)\n",
    "    \n",
    "    # calculate improvement percentage\n",
    "    model_names = list(results.keys())\n",
    "    if len(model_names) >= 2:\n",
    "        model1, model2 = model_names[0], model_names[1]\n",
    "        improvement = abs(results[model1] - results[model2])\n",
    "        percent_change = (improvement / min(results[model1], results[model2])) * 100\n",
    "        \n",
    "        print(\"\\nComparison Summary:\")\n",
    "        print(f\"Absolute difference in AUC: {improvement:.4f}\")\n",
    "        print(f\"Percent improvement: {percent_change:.2f}%\")\n",
    "        print(f\"Best performing model: {best_model} (AUC: {results[best_model]:.4f})\")\n",
    "        \n",
    "        # feature impact insight\n",
    "        if best_model == \"Random Forest\":\n",
    "            print(\"\\nInsight: Random Forest often performs better when there are complex interactions\")\n",
    "            print(\"between features that aren't explicitly engineered.\")\n",
    "        else:\n",
    "            print(\"\\nInsight: XGBoost often performs better when there's a clear gradient\")\n",
    "            print(\"structure in the data that benefits from sequential improvement.\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de423588",
   "metadata": {},
   "source": [
    "## 8. Save Results\n",
    "\n",
    "Save the AUC scores to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c719be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(model_results, file_path='results/results_part2.txt'):\n",
    "    # create 'results' directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # format results as strings\n",
    "    best_model = max(model_results, key=model_results.get)\n",
    "    lines = [\n",
    "        \"MODEL COMPARISON RESULTS\",\n",
    "        \"=\" * 40\n",
    "    ]\n",
    "    \n",
    "    # add each model's AUC score\n",
    "    for model_name, auc_score in model_results.items():\n",
    "        lines.append(f\"{model_name} AUC: {auc_score:.4f}\")\n",
    "    \n",
    "    # calculate improvement\n",
    "    if len(model_results) >= 2:\n",
    "        model_names = list(model_results.keys())\n",
    "        model1, model2 = model_names[0], model_names[1]\n",
    "        improvement = abs(model_results[model1] - model_results[model2])\n",
    "        percent_change = (improvement / min(model_results[model1], model_results[model2])) * 100\n",
    "        \n",
    "        lines.extend([\n",
    "            \"\\nCOMPARISON SUMMARY\",\n",
    "            \"=\" * 40,\n",
    "            f\"Absolute difference in AUC: {improvement:.4f}\",\n",
    "            f\"Percent improvement: {percent_change:.2f}%\",\n",
    "            f\"Best performing model: {best_model}\"\n",
    "        ])\n",
    "        \n",
    "        # add insights about time-series features\n",
    "        lines.extend([\n",
    "            \"\\nFEATURE ENGINEERING INSIGHTS\",\n",
    "            \"=\" * 40,\n",
    "            \"Adding time-series features (rolling mean and standard deviation)\",\n",
    "            \"allowed the models to capture temporal patterns in heart rate data,\",\n",
    "            \"potentially improving their predictive performance compared to\",\n",
    "            \"models using only static features.\"\n",
    "        ])\n",
    "    \n",
    "    # add timestamp\n",
    "    lines.append(f\"\\nEvaluation completed on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # write results to file\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write('\\n'.join(lines))\n",
    "    \n",
    "    print(f\"\\nResults saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3fd6ae",
   "metadata": {},
   "source": [
    "## 9. Main Execution\n",
    "\n",
    "Run the complete workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f733967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7326 records with 10 features.\n",
      "Date range: 2023-01-01 00:00:00 to 2023-08-05 13:16:36.634428\n",
      "Number of unique patients: 150\n",
      "Heart rate range: 40.0 to 120.9524593747634 bpm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mihir\\AppData\\Local\\Temp\\ipykernel_27384\\1213384138.py:34: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  df_sorted['hr_rolling_mean'] = df_sorted.groupby('patient_id')['hr_rolling_mean'].fillna(method='ffill')\n",
      "C:\\Users\\mihir\\AppData\\Local\\Temp\\ipykernel_27384\\1213384138.py:34: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_sorted['hr_rolling_mean'] = df_sorted.groupby('patient_id')['hr_rolling_mean'].fillna(method='ffill')\n",
      "C:\\Users\\mihir\\AppData\\Local\\Temp\\ipykernel_27384\\1213384138.py:35: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  df_sorted['hr_rolling_std'] = df_sorted.groupby('patient_id')['hr_rolling_std'].fillna(method='ffill')\n",
      "C:\\Users\\mihir\\AppData\\Local\\Temp\\ipykernel_27384\\1213384138.py:35: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_sorted['hr_rolling_std'] = df_sorted.groupby('patient_id')['hr_rolling_std'].fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated rolling features with 300 second window:\n",
      "  - hr_rolling_mean range: 40.0 to 121.0\n",
      "  - hr_rolling_std range: 1.0 to 14.7\n",
      "  Missing values after processing: 0\n",
      "Training set: 5860 samples with 11 features\n",
      "Testing set: 1466 samples with 11 features\n",
      "Missing values before imputation:\n",
      "  Training set: 600\n",
      "  Testing set: 135\n",
      "Missing values after imputation:\n",
      "  Training set: 0\n",
      "  Testing set: 0\n",
      "Random Forest Model Trained Successfully\n",
      "\n",
      "Top 5 important features:\n",
      "1. diastolic_bp: 0.2088\n",
      "2. glucose_level: 0.1779\n",
      "3. hr_rolling_std: 0.1528\n",
      "4. age: 0.1126\n",
      "5. hr_rolling_mean: 0.1049\n",
      "XGBoost Model Trained Successfully\n",
      "\n",
      "Top 5 important features:\n",
      "1. hr_rolling_mean: 0.2229\n",
      "2. diastolic_bp: 0.1944\n",
      "3. glucose_level: 0.1290\n",
      "4. heart_rate: 0.1170\n",
      "5. smoker_former: 0.0555\n",
      "Model Comparison:\n",
      "========================================\n",
      "Random Forest AUC: 0.9947\n",
      "XGBoost AUC: 1.0000\n",
      "\n",
      "Comparison Summary:\n",
      "Absolute difference in AUC: 0.0053\n",
      "Percent improvement: 0.53%\n",
      "Best performing model: XGBoost (AUC: 1.0000)\n",
      "\n",
      "Insight: XGBoost often performs better when there's a clear gradient\n",
      "structure in the data that benefits from sequential improvement.\n",
      "\n",
      "Results saved to results/results_part2.txt\n",
      "\n",
      "Feature Engineering Impact:\n",
      "The addition of time-series features (hr_rolling_mean and hr_rolling_std)\n",
      "helps the models capture temporal patterns in the heart rate data,\n",
      "providing context about patient health trends over time rather than\n",
      "just point-in-time measurements.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mihir\\OneDrive\\Desktop\\DataSci223\\5-put-a-label-on-it-MihirUCSF\\venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [11:20:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # load data\n",
    "    data_file = 'data/synthetic_health_data.csv'\n",
    "    df = load_data(data_file)\n",
    "    \n",
    "    # extract rolling features\n",
    "    window_size = 300  # 5 minutes in seconds\n",
    "    df_with_features = extract_rolling_features(df, window_size)\n",
    "    \n",
    "    # prepare data\n",
    "    X_train, X_test, y_train, y_test = prepare_data_part2(df_with_features)\n",
    "    \n",
    "    # train models\n",
    "    rf_model = train_random_forest(X_train, y_train)\n",
    "    xgb_model = train_xgboost(X_train, y_train)\n",
    "    \n",
    "    # compare models\n",
    "    models = {\n",
    "        \"Random Forest\": rf_model,\n",
    "        \"XGBoost\": xgb_model\n",
    "    }\n",
    "    \n",
    "    # use our compare_models function\n",
    "    model_results = compare_models(models, X_test, y_test)\n",
    "    \n",
    "    # save results\n",
    "    save_results(model_results)\n",
    "    \n",
    "    print(\"\\nFeature Engineering Impact:\")\n",
    "    print(\"The addition of time-series features (hr_rolling_mean and hr_rolling_std)\")\n",
    "    print(\"helps the models capture temporal patterns in the heart rate data,\")\n",
    "    print(\"providing context about patient health trends over time rather than\")\n",
    "    print(\"just point-in-time measurements.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
